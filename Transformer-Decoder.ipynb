{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code adapted from Transformer model for language understanding example:\n",
    "https://www.tensorflow.org/tutorials/text/transformer\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import nltk  # nltk.download('punkt')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text\n",
    "file_paths = ['The Old Man and the Sea.txt', 'The Sun Also Rises.txt', 'A Farewell to Arms.txt']\n",
    "text = \"\"\n",
    "for f in file_paths:\n",
    "    text += open('text/'+f, 'r', encoding='utf-8-sig').read().strip()\n",
    "\n",
    "text = re.sub(r'[_*]', '', text)\n",
    "text = re.sub(r'\\s+', ' ', text)  # \" \" can also be used to indicate spaces.\n",
    "print('The set of characters: ', sorted(set(text)))\n",
    "\n",
    "# Tokenization\n",
    "sentences = nltk.tokenize.sent_tokenize(text)  # Split text into sentences.\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(sentences, target_vocab_size=2**13)\n",
    "tokenized_sentences = [[tokenizer.vocab_size] + tokenizer.encode(s) + [tokenizer.vocab_size+1] for s in sentences]\n",
    "print('Number of sentences: ', len(tokenized_sentences))\n",
    "\n",
    "# Plot the distribution of sentence length.\n",
    "plot_length_distribution = False\n",
    "if plot_length_distribution:\n",
    "    fig, axs = plt.subplots()\n",
    "    axs.hist(list(map(len, tokenized_sentences)), 20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def filter_by_max(_list, _max_length):\n",
    "    return list(filter(lambda x: len(x) <= _max_length, _list))\n",
    "\n",
    "\n",
    "# Limit sentence length\n",
    "max_length = 40\n",
    "tokenized_sentences = filter_by_max(tokenized_sentences, max_length)\n",
    "data_size = len(tokenized_sentences)\n",
    "\n",
    "# Shift by one position\n",
    "data_input = [s[:-1] for s in tokenized_sentences]\n",
    "data_output = [s[1:] for s in tokenized_sentences]\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_size = (data_size * 90) // 100\n",
    "train_indices = [*range(data_size)]\n",
    "random.shuffle(train_indices)\n",
    "train_input = [data_input[i] for i in train_indices[:train_size]]\n",
    "train_output = [data_output[i] for i in train_indices[:train_size]]\n",
    "valid_input = [data_input[i] for i in train_indices[train_size:]]\n",
    "valid_output = [data_output[i] for i in train_indices[train_size:]]\n",
    "\n",
    "\n",
    "# Tensorflow Dataset\n",
    "batch_size = 64\n",
    "buffer_size = train_size\n",
    "num_epochs = 0\n",
    "\n",
    "train_input = tf.keras.preprocessing.sequence.pad_sequences(train_input, padding='post')\n",
    "train_output = tf.keras.preprocessing.sequence.pad_sequences(train_output, padding='post')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_output))\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(buffer_size)\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_input = tf.keras.preprocessing.sequence.pad_sequences(valid_input, padding='post')\n",
    "valid_output = tf.keras.preprocessing.sequence.pad_sequences(valid_output, padding='post')\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_input, valid_output))\n",
    "valid_dataset = valid_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "embedding_size = 128\n",
    "vocab_size = tokenizer.vocab_size + 2  # +2 is for start and end tokens.\n",
    "feed_forward_size = 512\n",
    "num_layers = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "# Positional Encoding\n",
    "def positional_encoding(_seq_length, _embedding_size):\n",
    "    t = np.arange(0, _seq_length)\n",
    "    t = t[:, np.newaxis]\n",
    "    d = np.arange(0, _embedding_size) // 2\n",
    "    omegas = 1 / (10000 ** (2 * d / _embedding_size))\n",
    "    omegas = omegas[np.newaxis, :]\n",
    "    radients = t * omegas\n",
    "    radients[:, 0::2] = np.sin(radients[:, 0::2])\n",
    "    radients[:, 1::2] = np.cos(radients[:, 1::2])\n",
    "    _positional_encoding = radients[np.newaxis, ...]\n",
    "    return tf.cast(_positional_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension,\n",
    "    i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "        q: query shape == (..., seq_length_q, depth_q)\n",
    "        k: key shape ==   (..., seq_length_k, depth_k)\n",
    "        v: value shape == (..., seq_length_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable\n",
    "              to (..., seq_length_q, seq_length_k). Defaults to None.\n",
    "\n",
    "    Restrictions:\n",
    "        depth_q = depth_k\n",
    "        seq_length_k = seq_length_v\n",
    "\n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_length_q, seq_length_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_length_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_length_q, seq_length_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_length_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, _embedding_size, _num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = _num_heads\n",
    "        self.embedding_size = _embedding_size\n",
    "\n",
    "        assert _embedding_size % self.num_heads == 0\n",
    "\n",
    "        self.depth = _embedding_size // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(_embedding_size)\n",
    "        self.wk = tf.keras.layers.Dense(_embedding_size)\n",
    "        self.wv = tf.keras.layers.Dense(_embedding_size)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(_embedding_size)\n",
    "\n",
    "    def split_heads(self, x, _batch_size):\n",
    "        \"\"\"The shape of input x is (batch_size, seq_length, embedding_size).\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_length, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (_batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        _batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_length_q, depth_q)\n",
    "        k = self.wk(k)  # (batch_size, seq_length_k, depth_k)\n",
    "        v = self.wv(v)  # (batch_size, seq_length_v, depth_v)\n",
    "\n",
    "        q = self.split_heads(q, _batch_size)  # (batch_size, num_heads, seq_length_q, depth_q)\n",
    "        k = self.split_heads(k, _batch_size)  # (batch_size, num_heads, seq_length_k, depth_k)\n",
    "        v = self.split_heads(v, _batch_size)  # (batch_size, num_heads, seq_length_v, depth_v)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_length_q, depth_v)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_length_q, seq_length_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # (batch_size, seq_length_q, num_heads, depth_v)\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=[0, 2, 1, 3])\n",
    "\n",
    "        # (batch_size, seq_length_q, embedding_size)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (_batch_size, -1, self.embedding_size))\n",
    "\n",
    "        # output.shape == (batch_size, seq_length_q, embedding_size)\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(_embedding_size, _feed_forward_size):\n",
    "    return tf.keras.Sequential([\n",
    "        # Dense.shape == (batch_size, seq_length, feed_forward_size)\n",
    "        tf.keras.layers.Dense(_feed_forward_size, activation='relu'),\n",
    "        # Dense.shape == (batch_size, seq_length, embedding_size)\n",
    "        tf.keras.layers.Dense(_embedding_size)\n",
    "    ])\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, _embedding_size, _num_heads, _feed_forward_size, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(_embedding_size, _num_heads)\n",
    "\n",
    "        self.feed_forward = point_wise_feed_forward_network(_embedding_size, _feed_forward_size)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, _look_ahead_mask):\n",
    "        # attn1.shape == (batch_size, seq_length, embedding_size)\n",
    "        attn1, attn_weights_block1 = self.mha(x, x, x, _look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # feed_forward_output.shape == (batch_size, seq_length, embedding_size)\n",
    "        feed_forward_output = self.feed_forward(out1)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
    "        # out2.shape == (batch_size, seq_length, embedding_size)\n",
    "        out2 = self.layernorm2(feed_forward_output + out1)\n",
    "\n",
    "        return out2, attn_weights_block1\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.Model):\n",
    "    def __init__(self, _num_layers, _embedding_size, _num_heads, _feed_forward_size, _vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = _embedding_size\n",
    "        self.num_layers = _num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(_vocab_size, _embedding_size)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, _embedding_size)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(_embedding_size, _num_heads, _feed_forward_size, rate)\n",
    "                           for _ in range(_num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(_vocab_size)\n",
    "\n",
    "    def call(self, x, training, _look_ahead_mask):\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, embedding_size)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_size, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_length, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1 = self.dec_layers[i](x, training, _look_ahead_mask)\n",
    "            attention_weights['decoder_layer{}'.format(i + 1)] = block1\n",
    "\n",
    "        # x.shape == (batch_size, seq_length, embedding_size)\n",
    "        # block1.shape == (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        final_output = self.final_layer(x)  # (batch_size, tar_seq_len, vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sample_transformerdecoder = TransformerDecoder(num_layers, embedding_size,\n",
    "                                               num_heads, feed_forward_size, vocab_size,\n",
    "                                               maximum_position_encoding=vocab_size, rate=0.1)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformerdecoder(temp_input, training=False,\n",
    "                                      _look_ahead_mask=None)\n",
    "\n",
    "print(fn_out.shape)  # (batch_size, seq_length, vocab_size)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, _embedding_size, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = _embedding_size\n",
    "        self.embedding_size = tf.cast(self.embedding_size, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.embedding_size) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(embedding_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\"\"\"\n",
    "temp_learning_rate_schedule = CustomSchedule(embedding_size)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "\"\"\"\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "valid_loss_history = []\n",
    "valid_accuracy_history = []\n",
    "\n",
    "\n",
    "transformer_decoder = TransformerDecoder(num_layers, embedding_size,\n",
    "                                         num_heads, feed_forward_size, vocab_size,\n",
    "                                         maximum_position_encoding=vocab_size, rate=0.1)\n",
    "\n",
    "\n",
    "def create_masks(x):\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(x)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "\n",
    "# Create and load checkpoints\n",
    "checkpoint_dir = 'training_td'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 transformer_decoder=transformer_decoder)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint,\n",
    "                                                checkpoint_dir,\n",
    "                                                max_to_keep=3,\n",
    "                                                checkpoint_name='ckpoint')\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    # checkpoint_manager.latest_checkpoint is equivalent to\n",
    "    # tf.train.latest_checkpoint(directory)\n",
    "    print('Latest checkpoint files are successfully restored.')\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def train_step(_inp, _tar):\n",
    "    combined_mask = create_masks(_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer_decoder(_inp,\n",
    "                                             True,\n",
    "                                             combined_mask)\n",
    "        loss = loss_function(_tar, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer_decoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer_decoder.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_accuracy.update_state(_tar, predictions)\n",
    "\n",
    "\n",
    "def valid_step(_inp, _tar):\n",
    "    combined_mask = create_masks(_inp)\n",
    "    predictions, _ = transformer_decoder(_inp,\n",
    "                                         False,\n",
    "                                         combined_mask)\n",
    "    loss = loss_function(_tar, predictions)\n",
    "\n",
    "    valid_loss.update_state(loss)\n",
    "    valid_accuracy.update_state(_tar, predictions)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Training Loss {:.4f} Training Accuracy {:.4f}'.format(\n",
    "                   epoch + 1, batch, train_loss.result().numpy(), train_accuracy.result().numpy()))\n",
    "\n",
    "    train_loss_history.append(train_loss.result().numpy())\n",
    "    train_accuracy_history.append(train_accuracy.result().numpy())\n",
    "\n",
    "    # Validation\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(valid_dataset):\n",
    "        valid_step(inp, tar)\n",
    "\n",
    "    valid_loss_history.append(valid_loss.result().numpy())\n",
    "    valid_accuracy_history.append(valid_accuracy.result().numpy())\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_save_path = checkpoint_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1,\n",
    "                                                            checkpoint_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                        train_loss.result().numpy(),\n",
    "                                                        train_accuracy.result().numpy()))\n",
    "\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# Plot model loss and accuracy\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "x_epoch = np.arange(len(train_loss_history)) + 1\n",
    "axs[0].plot(x_epoch, train_loss_history, linewidth=2, label='Training')\n",
    "axs[0].plot(x_epoch, valid_loss_history, '--', linewidth=2, label='Validation')\n",
    "axs[0].set_xlabel('Epoch', size=15)\n",
    "axs[0].set_ylabel('Loss', size=15)\n",
    "axs[1].plot(x_epoch, train_accuracy_history, linewidth=2, label='Training')\n",
    "axs[1].plot(x_epoch, valid_accuracy_history, '--', linewidth=2, label='Validation')\n",
    "axs[1].set_xlabel('Epoch', size=15)\n",
    "axs[1].set_ylabel('Accuracy', size=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('td_loss_accuracy.pdf')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with open(\"train_accuracy_history.txt\", \"w\") as f:\n",
    "    for s in train_accuracy_history:\n",
    "        f.write(str(s) +\"\\n\")\n",
    "\n",
    "with open(\"train_loss_history.txt\", \"w\") as f:\n",
    "    for s in train_loss_history:\n",
    "        f.write(str(s) +\"\\n\")\n",
    "\n",
    "with open(\"valid_accuracy_history.txt\", \"w\") as f:\n",
    "    for s in valid_accuracy_history:\n",
    "        f.write(str(s) + \"\\n\")\n",
    "\n",
    "with open(\"valid_loss_history.txt\", \"w\") as f:\n",
    "    for s in valid_loss_history:\n",
    "        f.write(str(s) + \"\\n\")\n",
    "\n",
    "with open(\"train_accuracy_history.txt\", \"r\") as f:\n",
    "    for s in f:\n",
    "        train_accuracy_history.append(float(s.strip()))\n",
    "\n",
    "with open(\"train_loss_history.txt\", \"r\") as f:\n",
    "    for s in f:\n",
    "        train_loss_history.append(float(s.strip()))\n",
    "\n",
    "with open(\"valid_accuracy_history.txt\", \"r\") as f:\n",
    "    for s in f:\n",
    "        valid_accuracy_history.append(float(s.strip()))\n",
    "\n",
    "with open(\"valid_loss_history.txt\", \"r\") as f:\n",
    "    for s in f:\n",
    "        valid_loss_history.append(float(s.strip()))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Text Generation and Attention Plot\n",
    "def text_generator(_model, start, temperature=1.0, plot_attention=False):\n",
    "    encoded_text = [tokenizer.vocab_size] + tokenizer.encode(start)\n",
    "    encoded_text = tf.expand_dims(encoded_text, 0)  # Expand by adding batch and time dimensions.\n",
    "    max_iteration = tokenizer.vocab_size + 2 - len(encoded_text[0])\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        combined_mask = create_masks(encoded_text)\n",
    "        prediction, attention = _model(encoded_text,\n",
    "                                       False,\n",
    "                                       combined_mask)\n",
    "        prediction = tf.squeeze(prediction, 0)\n",
    "        prediction = prediction / temperature\n",
    "        prediction = tf.random.categorical(prediction, num_samples=1)[-1, 0]\n",
    "        prediction = tf.squeeze(prediction).numpy()\n",
    "        prediction = tf.expand_dims([prediction], 0)\n",
    "        encoded_text = tf.concat([encoded_text, prediction], axis=-1)\n",
    "\n",
    "        if tf.squeeze(prediction) == tokenizer.vocab_size+1 or tf.squeeze(prediction) == 0:\n",
    "            encoded_text = tf.squeeze(encoded_text)\n",
    "            if plot_attention:\n",
    "                plot_attention_weights(attention, encoded_text, 'decoder_layer2')\n",
    "            return tokenizer.decode([sw for sw in encoded_text if sw < tokenizer.vocab_size])\n",
    "\n",
    "    encoded_text = tf.squeeze(encoded_text)\n",
    "    if plot_attention:\n",
    "        plot_attention_weights(attention, encoded_text, 'decoder_layer2')\n",
    "    return tokenizer.decode([sw for sw in encoded_text if sw < tokenizer.vocab_size])\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, _encoded_text, layer):\n",
    "    fig = plt.figure(figsize=(15, 7.5))  # Width and Height\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 9}\n",
    "\n",
    "        ax.set_xticks(range(len(_encoded_text) - 1))\n",
    "        ax.set_yticks(range(len(_encoded_text) - 1))\n",
    "\n",
    "        ax.set_xticklabels(['<start>'] + [tokenizer.decode([i]) for i in _encoded_text[1:-1]],\n",
    "                           fontdict=fontdict, rotation=90)\n",
    "\n",
    "        eos = ['<end>'] if _encoded_text[-1] == tokenizer.vocab_size+1 else [tokenizer.decode([_encoded_text[-1]])]\n",
    "        ax.set_yticklabels([tokenizer.decode([i]) for i in _encoded_text[1:-1]] + eos,\n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('attention.pdf')\n",
    "\n",
    "\n",
    "# Generate a text from the model\n",
    "print(text_generator(transformer_decoder, 'I ', 0.9, True))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_python37]",
   "language": "python",
   "name": "conda-env-tf_python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
